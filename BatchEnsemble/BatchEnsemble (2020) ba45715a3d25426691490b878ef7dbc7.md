# BatchEnsemble (2020)

Created At: 2021년 8월 12일 오전 10:59
Created By: 조승제
Topics: Deep Learning
Type: 📒 Lesson
발표일: 2021년 8월 12일
발표자: 조승제
참석자: 조건우, 유정민

# 1. Introduction

- Deep neural networks trained with different random seeds can converge to very different local minima although they share similar error rates
- 비슷한 error rate를 보여도 다른 error가 나옴
- Computational, Memory Cost가 Ensemble하는 모델(member)의 수에 따라 선형적으로 증가함
    - Computational Cost : 각 member마다 개별적인 forward pass가 필요
    - Memory Cost : member 각각의 weight를 모두 저장해야 함
- we aim to address the computational and memory bottleneck by building a more parameter efficient-ensemble method: BatchEnsemble.

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled.png)

- device간 병렬화 뿐만 아니라 device 내에서 병렬화도 가능
- 또한 ensemble member 간에 많은 weights를 공유하기 때문에 메모리 오버헤드가 적음
- Additionally, we show that BatchEnsemble is effective in calibrated prediction on out-of-distribution datasets

---

# 2. Background

1. Ensembles for improved performance
    1. Bagging : variance 감소
    2. Boosting : bias 감소
2. Ensembles for improved uncertainty
    1. ex) Bayesian NNs
3. Lifelong learning
    - 인간의 뇌는 새로운 것을 배워도 과거의 지식을 잘 잊어버리지 않는데, 이를 모방하는 메커니즘
    - 현재 AI는 새로운 지식을 학습하면 과거의 내용을 잊어버리는 치명적인 문제가 있음
        - Catastrophic forgetting(파괴적 망각)
        - Semantic drift(의미 변화)

---

# 3. Methods

## 3.1 Batch Ensemble

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%201.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%201.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%202.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%202.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%203.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%203.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%204.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%204.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%205.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%205.png)

$W$ = the weights in a neural network layer (shared weight)

$\overline{W_i}$ = each ensemble member has weight matrix

$m$ = input dimension, $n$ = output dimension

$r_i, s_i$ = Each ensemble member owns a tuple of trainable vectors, the same dimension as input and output ($m$ and $n$)

$M$ = ensemble size

$i$ = ranges from 1 to $M$

$x$ = the activations of thr incoming neurons in a neural network layer

$n$ = mini-batch index

$X$ = mini-batch input

- Member가 추가되더라도 matrix가 아닌 $r_i, s_i$ vector가 추가되므로 memory cost 감소
- Input mini-batch가 8일 때, ensemble M = 4이면 Sub-batch size = 2가 되고, 각각의 sub-batch에는 ensemble weight를 받음 → 4개의 ensemble member가 동시에 학습

## 3.2 Computational Cost

- 단일 신경망과 비교해서 BatchEnsemble은 Hadamard product만 추가되기 때문에, computational overhead가 거의 없음
- One limitation of BatchEnsemble is that if we keep the minibatch size the same as single model training, each ensemble member gets only a portion of input data
- In practice, the above issue can be remedied by increasing the batch size so that each ensemble member receives the same amount of data as ordinary single model training
- hardware that can fully utilize large batch size.

## 3.3 Batch ensemble as an approach to lifelong learning

- 훈련 방법

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%206.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%206.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%207.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%207.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%208.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%208.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%209.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%209.png)

- 한계점
    - 각 Task가 rank-1 matrix로만 학습됨 → task 차이가 큰 경우 제대로 학습 x
    - shared weight가 첫 번째 Task에서만 학습됨
    - 해결책은 논문에 없고 향후 과제로 남김

---

# 4. Experiments

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2010.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2010.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2011.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2011.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2012.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2012.png)