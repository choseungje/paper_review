# BatchEnsemble (2020)

Created At: 2021ë…„ 8ì›” 12ì¼ ì˜¤ì „ 10:59
Created By: ì¡°ìŠ¹ì œ
Topics: Deep Learning
Type: ğŸ“’ Lesson
ë°œí‘œì¼: 2021ë…„ 8ì›” 12ì¼
ë°œí‘œì: ì¡°ìŠ¹ì œ
ì°¸ì„ì: ì¡°ê±´ìš°, ìœ ì •ë¯¼

# 1. Introduction

- Deep neural networks trained with different random seeds can converge to very different local minima although they share similar error rates
- ë¹„ìŠ·í•œ error rateë¥¼ ë³´ì—¬ë„ ë‹¤ë¥¸ errorê°€ ë‚˜ì˜´
- Computational, Memory Costê°€ Ensembleí•˜ëŠ” ëª¨ë¸(member)ì˜ ìˆ˜ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•¨
    - Computational Cost : ê° memberë§ˆë‹¤ ê°œë³„ì ì¸ forward passê°€ í•„ìš”
    - Memory Cost : member ê°ê°ì˜ weightë¥¼ ëª¨ë‘ ì €ì¥í•´ì•¼ í•¨
- we aim to address the computational and memory bottleneck by building a more parameter efficient-ensemble method: BatchEnsemble.

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled.png)

- deviceê°„ ë³‘ë ¬í™” ë¿ë§Œ ì•„ë‹ˆë¼ device ë‚´ì—ì„œ ë³‘ë ¬í™”ë„ ê°€ëŠ¥
- ë˜í•œ ensemble member ê°„ì— ë§ì€ weightsë¥¼ ê³µìœ í•˜ê¸° ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œê°€ ì ìŒ
- Additionally, we show that BatchEnsemble is effective in calibrated prediction on out-of-distribution datasets

---

# 2. Background

1. Ensembles for improved performance
    1. Bagging : variance ê°ì†Œ
    2. Boosting : bias ê°ì†Œ
2. Ensembles for improved uncertainty
    1. ex) Bayesian NNs
3. Lifelong learning
    - ì¸ê°„ì˜ ë‡ŒëŠ” ìƒˆë¡œìš´ ê²ƒì„ ë°°ì›Œë„ ê³¼ê±°ì˜ ì§€ì‹ì„ ì˜ ìŠì–´ë²„ë¦¬ì§€ ì•ŠëŠ”ë°, ì´ë¥¼ ëª¨ë°©í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜
    - í˜„ì¬ AIëŠ” ìƒˆë¡œìš´ ì§€ì‹ì„ í•™ìŠµí•˜ë©´ ê³¼ê±°ì˜ ë‚´ìš©ì„ ìŠì–´ë²„ë¦¬ëŠ” ì¹˜ëª…ì ì¸ ë¬¸ì œê°€ ìˆìŒ
        - Catastrophic forgetting(íŒŒê´´ì  ë§ê°)
        - Semantic drift(ì˜ë¯¸ ë³€í™”)

---

# 3. Methods

## 3.1 Batch Ensemble

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%201.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%201.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%202.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%202.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%203.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%203.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%204.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%204.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%205.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%205.png)

$W$ = the weights in a neural network layer (shared weight)

$\overline{W_i}$ = each ensemble member has weight matrix

$m$ = input dimension, $n$ = output dimension

$r_i, s_i$ = Each ensemble member owns a tuple of trainable vectors, the same dimension as input and output ($m$ and $n$)

$M$ = ensemble size

$i$ = ranges from 1 to $M$

$x$ = the activations of thr incoming neurons in a neural network layer

$n$ = mini-batch index

$X$ = mini-batch input

- Memberê°€ ì¶”ê°€ë˜ë”ë¼ë„ matrixê°€ ì•„ë‹Œ $r_i, s_i$ vectorê°€ ì¶”ê°€ë˜ë¯€ë¡œ memory cost ê°ì†Œ
- Input mini-batchê°€ 8ì¼ ë•Œ, ensemble M = 4ì´ë©´ Sub-batch size = 2ê°€ ë˜ê³ , ê°ê°ì˜ sub-batchì—ëŠ” ensemble weightë¥¼ ë°›ìŒ â†’ 4ê°œì˜ ensemble memberê°€ ë™ì‹œì— í•™ìŠµ

## 3.2 Computational Cost

- ë‹¨ì¼ ì‹ ê²½ë§ê³¼ ë¹„êµí•´ì„œ BatchEnsembleì€ Hadamard productë§Œ ì¶”ê°€ë˜ê¸° ë•Œë¬¸ì—, computational overheadê°€ ê±°ì˜ ì—†ìŒ
- One limitation of BatchEnsemble is that if we keep the minibatch size the same as single model training, each ensemble member gets only a portion of input data
- In practice, the above issue can be remedied by increasing the batch size so that each ensemble member receives the same amount of data as ordinary single model training
- hardware that can fully utilize large batch size.

## 3.3 Batch ensemble as an approach to lifelong learning

- í›ˆë ¨ ë°©ë²•

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%206.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%206.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%207.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%207.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%208.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%208.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%209.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%209.png)

- í•œê³„ì 
    - ê° Taskê°€ rank-1 matrixë¡œë§Œ í•™ìŠµë¨ â†’ task ì°¨ì´ê°€ í° ê²½ìš° ì œëŒ€ë¡œ í•™ìŠµ x
    - shared weightê°€ ì²« ë²ˆì§¸ Taskì—ì„œë§Œ í•™ìŠµë¨
    - í•´ê²°ì±…ì€ ë…¼ë¬¸ì— ì—†ê³  í–¥í›„ ê³¼ì œë¡œ ë‚¨ê¹€

---

# 4. Experiments

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2010.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2010.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2011.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2011.png)

![BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2012.png](BatchEnsemble%20(2020)%20ba45715a3d25426691490b878ef7dbc7/Untitled%2012.png)